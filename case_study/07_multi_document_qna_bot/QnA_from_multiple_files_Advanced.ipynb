{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrxX8KIFZpbTC+dv7/nNGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edquestofficial/RAG-With-LLama-Index/blob/main/QnA_from_multiple_files_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1> QnA From Multiple Files Advanced </h1>**\n",
        "\n",
        "This project allows users to extract information and get answers from any uploaded file, whether it's a text file, PDF, or any other type.\n",
        "\n",
        "**<h2> Features </h2>**\n",
        "\n",
        "*   Multi-Format Support: Extract information from text files, PDFs, and other document types.\n",
        "*  Flexible Querying: Retrieve answers and insights based on user queries.\n",
        "\n",
        "*   Seamless Integration: Utilize Chroma and LlamaIndex for efficient document indexing and querying."
      ],
      "metadata": {
        "id": "_ICaDac3WU75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Step 1:\n",
        "Install the necessary dependencies:"
      ],
      "metadata": {
        "id": "OK7j0aJFXTIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index.core\n",
        "!pip install llama_index.llms.gemini\n",
        "!pip install llama_index.embeddings.gemini\n",
        "!pip install llama_index.vector_stores.chroma"
      ],
      "metadata": {
        "id": "z3bkU4UJAubM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "!pip install llama-index-readers-file"
      ],
      "metadata": {
        "id": "CisonfwCBo7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Step 2:**\n",
        "Integrating LlamaIndex, Gemini, and Chroma </h4>"
      ],
      "metadata": {
        "id": "a_kAI4aEajqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "rv542qBkAoL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Step 3:**\n",
        "Configuring Gemini Models with API Key"
      ],
      "metadata": {
        "id": "KG2tLJzHa58o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = input(\"enter api key: \")\n",
        "\n",
        "gemini_embedding_model = GeminiEmbedding(api_key=api_key, model_name=\"models/embedding-001\")\n",
        "\n",
        "llm = Gemini(api_key=api_key, model_name=\"models/gemini-pro\")"
      ],
      "metadata": {
        "id": "DktwfNteBmJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Step 4:**\n",
        "File Upload and Directory Setup"
      ],
      "metadata": {
        "id": "4I5ffNvbbWX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create a directory if not existing\n",
        "data_dir = 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Move uploaded files to the data directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, os.path.join(data_dir, filename))"
      ],
      "metadata": {
        "id": "Ow5gWua7B668"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Step 5:**\n",
        "Index Creation and Storage Setup with Chroma and LlamaIndex"
      ],
      "metadata": {
        "id": "ckbDhikubzj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(data_dir).load_data()\n",
        "\n",
        "# Create a client and a new collection\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = client.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# Create a vector store\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create a storage context\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Set Global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = gemini_embedding_model\n",
        "\n",
        "# Create an index from the documents and save it to the disk\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
      ],
      "metadata": {
        "id": "CqoHF2LMCBFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Step 6:**\n",
        "Querying Index and Retrieving Results"
      ],
      "metadata": {
        "id": "zNf3dH4tcB4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Fetch the collection\n",
        "chroma_collection = load_client.get_collection(\"quickstart\")\n",
        "\n",
        "# Fetch the vector store\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Get the index from the vector store\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store\n",
        ")\n",
        "\n",
        "# Initialize the query engine (assuming 'index' is already created and available)\n",
        "test_query_engine = index.as_query_engine()\n",
        "response = test_query_engine.query(input('what is your question: '))\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PrL5GZGlCYOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
